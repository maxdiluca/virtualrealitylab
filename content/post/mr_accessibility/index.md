---
title: "MR for Accessibility: Augmenting Senses for Inclusive Virtual Worlds"
date: 2025-11-03T15:00:00+00:00
draft: false
featured: false
summary: Using mixed reality to adapt human sensing and interaction for more inclusive digital experiences.
---

Mixed Reality (MR) can adapt and extend human senses, letting people operate beyond real-world constraints and helping to level the playing field across physical, social, and environmental limitations. This post introduces the theme, outlines exemplar studies, and points to resources for designing inclusive MR systems. :contentReference[oaicite:0]{index=0}

![Figure 1: Overview of accessibility themes in MR.](images/figure_1.jpg)

## Why accessibility in MR matters
By decoupling experience from strictly physical laws, MR enables users to achieve tasks otherwise difficult or impossible, and supports participation by people with diverse abilities. The goal is not only access, but equitable agency, confidence, and performance in complex environments. :contentReference[oaicite:1]{index=1}

![Figure 2: Relationship between sensory adaptation and accessibility.](images/figure_2.jpg)

## Selected works and contributions

- **Navigation and agency for blind travelers.** *Glide* explores mode switching between device-directed and user-directed control, studying impacts on agency, trust, and performance with blind/low-vision participants (HRI ’23). This line of work contributed to the formation of **Gliadance.IO**. :contentReference[oaicite:2]{index=2}  

![Figure 3: Glide study—navigation and agency for blind travelers.](images/figure_3.jpg)

- **Expressive avatar motion from sparse input.** *CoolMoves* synthesizes accentuated full-body motion in real time from commodity VR signals using database matching and probabilistic smoothing (IMWUT ’21). :contentReference[oaicite:3]{index=3}  

![Figure 4: Example of expressive avatar motion synthesis.](images/figure_4.jpg)

- **Sound accessibility taxonomy for VR.** A two-dimension framework (source × intent) categorizes sounds across dozens of apps, informing visual/haptic sound substitutes for D/deaf and hard-of-hearing users (DIS 2021, Best Paper). :contentReference[oaicite:4]{index=4}  

![Figure 5: Taxonomy of sound accessibility in VR.](images/figure_5.jpg)

- **Unimanual→bimanual interaction mapping.** *Two-In-One* defines a design space that remaps limited unimanual input to bimanual interactions in VR (TACCESS). :contentReference[oaicite:5]{index=5}  

![Figure 6: Mapping of unimanual to bimanual interaction.](images/figure_6.jpg)

- **Toward sound accessibility in VR.** Empirical guidance for accessible auditory representations and alternatives (ICMI 2021). :contentReference[oaicite:6]{index=6}  

![Figure 7: Sound design for accessibility in virtual environments.](images/figure_7.jpg)

- **Movement projection and stylization.** *SnapMove* examines projected movement transformations to support performance and expression (AIVR 2020). :contentReference[oaicite:7]{index=7}  

![Figure 8: Stylization and projection of movement in MR.](images/figure_8.jpg)

- **Non-visual navigation in VR.** A haptic+auditory “white cane” enables navigation of complex virtual spaces without vision (CHI 2020, Honorable Mention). :contentReference[oaicite:8]{index=8}  

![Figure 9: Haptic and auditory navigation for non-visual users.](images/figure_9.jpg)

- **Low-vision tools for VR.** *SeeingVR* bundles techniques to improve readability, contrast, and guidance for low-vision users (CHI 2019). :contentReference[oaicite:9]{index=9}  

![Figure 10: Tools for low-vision accessibility in VR.](images/figure_10.jpg)

## Outlook
Accessibility in MR requires coordinated advances in sensing, semantic feedback (audio/visual/haptic), and interaction design. The literature above illustrates how control sharing, motion synthesis, alternative sensory channels, and adaptive tooling can increase agency and inclusion across user groups. :contentReference[oaicite:10]{index=10}
